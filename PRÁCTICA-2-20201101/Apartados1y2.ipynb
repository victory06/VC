{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Apartados1y2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXjcqow1J0ac"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "#########################################################################\n",
        "############ CARGAR LAS LIBRERÍAS NECESARIAS ############################\n",
        "#########################################################################\n",
        "\n",
        "# En caso de necesitar instalar keras en google colab,\n",
        "# ejecutar la siguiente línea:\n",
        "# !pip install -q keras\n",
        "# Importar librerías necesarias\n",
        "import numpy as np\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.utils as np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, GlobalAveragePooling2D, AveragePooling2D\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
        "from keras.losses import categorical_crossentropy\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import EarlyStopping\n",
        "# Optimizador a usar\n",
        "from keras.optimizers import SGD,Adam\n",
        "\n",
        "# Importar el conjunto de datos\n",
        "from keras.datasets import cifar100\n",
        "\n",
        "INPUT_SHAPE=(32, 32, 3)\n",
        "weights=None\n",
        "\n",
        "#########################################################################\n",
        "######## FUNCIÓN PARA CARGAR Y MODIFICAR EL CONJUNTO DE DATOS ###########\n",
        "#########################################################################\n",
        "\n",
        "# A esta función solo se la llama una vez. Devuelve 4 \n",
        "# vectores conteniendo, por este orden, las imágenes\n",
        "# de entrenamiento, las clases de las imágenes de\n",
        "# entrenamiento, las imágenes del conjunto de test y\n",
        "# las clases del conjunto de test.\n",
        "def cargarImagenes():\n",
        "    # Cargamos Cifar100. Cada imagen tiene tamaño\n",
        "    # (32 , 32, 3). Nos vamos a quedar con las\n",
        "    # imágenes de 25 de las clases.\n",
        "    (x_train, y_train), (x_test, y_test) = cifar100.load_data (label_mode ='fine')\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    train_idx = np.isin(y_train, np.arange(25))\n",
        "    train_idx = np.reshape (train_idx, -1)\n",
        "    x_train = x_train[train_idx]\n",
        "    y_train = y_train[train_idx]\n",
        "    test_idx = np.isin(y_test, np.arange(25))\n",
        "    test_idx = np.reshape(test_idx, -1)\n",
        "    x_test = x_test[test_idx]\n",
        "    y_test = y_test[test_idx]\n",
        "    \n",
        "    # Transformamos los vectores de clases en matrices.\n",
        "    # Cada componente se convierte en un vector de ceros\n",
        "    # con un uno en la componente correspondiente a la\n",
        "    # clase a la que pertenece la imagen. Este paso es\n",
        "    # necesario para la clasificación multiclase en keras.\n",
        "    y_train = np_utils.to_categorical(y_train, 25)\n",
        "    y_test = np_utils.to_categorical(y_test, 25)\n",
        "    \n",
        "    return x_train , y_train , x_test , y_test\n",
        "\n",
        "#########################################################################\n",
        "######## FUNCIÓN PARA OBTENER EL ACCURACY DEL CONJUNTO DE TEST ##########\n",
        "#########################################################################\n",
        "\n",
        "# Esta función devuelve la accuracy de un modelo, \n",
        "# definida como el porcentaje de etiquetas bien predichas\n",
        "# frente al total de etiquetas. Como parámetros es\n",
        "# necesario pasarle el vector de etiquetas verdaderas\n",
        "# y el vector de etiquetas predichas, en el formato de\n",
        "# keras (matrices donde cada etiqueta ocupa una fila,\n",
        "# con un 1 en la posición de la clase a la que pertenece y un 0 en las demás).\n",
        "def calcularAccuracy(labels, preds):\n",
        "    labels = np.argmax(labels, axis = 1)\n",
        "    preds = np.argmax(preds, axis = 1)\n",
        "    accuracy = sum(labels == preds)/len(labels)\n",
        "    return accuracy\n",
        "\n",
        "#########################################################################\n",
        "## FUNCIÓN PARA PINTAR LA PÉRDIDA Y EL ACCURACY EN TRAIN Y VALIDACIÓN ###\n",
        "#########################################################################\n",
        "\n",
        "# Esta función pinta dos gráficas, una con la evolución\n",
        "# de la función de pérdida en el conjunto de train y\n",
        "# en el de validación, y otra con la evolución de la\n",
        "# accuracy en el conjunto de train y el de validación.\n",
        "# Es necesario pasarle como parámetro el historial del\n",
        "# entrenamiento del modelo (lo que devuelven las\n",
        "# funciones fit() y fit_generator()).\n",
        "def mostrarEvolucion(hist):\n",
        "    plt.figure(1)\n",
        "    loss = hist.history['loss']\n",
        "    val_loss = hist.history['val_loss']\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training loss', 'Validation loss'])\n",
        "    plt.show()\n",
        "    \n",
        "    \n",
        "    plt.figure(2)\n",
        "    acc = hist.history['accuracy']\n",
        "    val_acc = hist.history['val_accuracy']\n",
        "    plt.plot(acc)\n",
        "    plt.plot(val_acc)\n",
        "    plt.legend(['Training accuracy','Validation accuracy'])\n",
        "    plt.show()\n",
        "\n",
        "#########################################################################\n",
        "################## DEFINICIÓN DEL MODELO BASENET ########################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\"Instancia un modelo básico de baseNet con la arquitectura que se muestra\n",
        "en la guía de prácticas.\"\"\"\n",
        "def basenetBasico():\n",
        "    #Modelo Sequential inicial\n",
        "    model = Sequential()\n",
        "    #Aquitectura siguiendo la guía de prácticas\n",
        "    model.add(Conv2D(6, kernel_size=(5, 5),\n",
        "                     activation='relu',\n",
        "                     input_shape=INPUT_SHAPE))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Conv2D(16,\n",
        "                     kernel_size = (5, 5),\n",
        "                     activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50,\n",
        "                    activation = 'relu'))\n",
        "    model.add(Dense(25,activation = 'softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "######### DEFINICIÓN DEL OPTIMIZADOR Y COMPILACIÓN DEL MODELO ###########\n",
        "#########################################################################\n",
        "\"\"\"Función para compilar el modelo. De optimizador elegimos SGD.\"\"\"\n",
        "def compilar(model):\n",
        "\n",
        "    # Definimos el optimizador\n",
        "    opt = SGD(lr = 0.01, decay = 1e-6,\n",
        "              momentum = 0.9, nesterov = True)\n",
        "\n",
        "    # Compilamos el modelo\n",
        "    model.compile(loss = categorical_crossentropy,\n",
        "                  optimizer = opt,\n",
        "                  metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Una vez tenemos el modelo base, y antes de entrenar, vamos a guardar los\n",
        "# pesos aleatorios con los que empieza la red, para poder reestablecerlos\n",
        "# después y comparar resultados entre no usar mejoras y sí usarlas.\n",
        "def guardarPesos(model):\n",
        "    weights = model.get_weights()\n",
        "    return weights\n",
        "\n",
        "#########################################################################\n",
        "###################### ENTRENAMIENTO DEL MODELO #########################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\"Función de entrenamiento del modelo model compilado. Si no se le pasa ningún\n",
        "datagen de ImageDataGenerator, se hace un entrenamiento normal con un\n",
        "10% de validación. Viene con parámetros por defecto para el tamaño del batch\n",
        "y las épocas. Devuelve el modelo entrenado, los datos de test (por si\n",
        "se quiere mostrar el accuray y el loss) y el historial de entrenamiento\"\"\"\n",
        "def entrenarModelo(model, datagen=\"foo\", batch_size=32, epocas=12):\n",
        "    #Guardamos los pesos por si reutilizamos el modelo\n",
        "    guardarPesos(model)\n",
        "    #Cargamos las imágenes de train y test\n",
        "    x_train , y_train , x_test , y_test=cargarImagenes()\n",
        "    if(datagen==\"foo\"):\n",
        "        #Entrenamos el modelo de forma normal si no hay datagen\n",
        "        historial = model.fit(x_train, y_train,\n",
        "                              validation_split=0.1,\n",
        "                              batch_size=batch_size,epochs=epocas,verbose=1)\n",
        "    else:\n",
        "        #Lista de callbacks para el early stopping\n",
        "        cb_list = []\n",
        "\n",
        "        # Paramos si no mejoramos en un número determinado de épocas\n",
        "        es_loss = EarlyStopping(monitor = 'val_loss',\n",
        "                                            patience = 15,\n",
        "                                            restore_best_weights = True)\n",
        "        es_acc = EarlyStopping(monitor = 'val_accuracy',\n",
        "                                          patience = 15,\n",
        "                                          restore_best_weights = True)\n",
        "\n",
        "        cb_list.append(es_loss)\n",
        "        cb_list.append(es_acc)\n",
        "        #Entrenamos modelo con el datagen dado por parámetro\n",
        "        #Primero aplicamos el preprocesamiento\n",
        "        datagen.fit(x_train)\n",
        "        #Misma normalización a los datos de test\n",
        "        datagen.standardize(x_test)\n",
        "        #Generamos el entrenamiento para fit_generator\n",
        "        generator_train=datagen.flow(x_train, y_train,\n",
        "                     batch_size = batch_size, subset ='training')   \n",
        "        #Generamos la validación para fit_generator\n",
        "        generator_valid=datagen.flow(x_train, y_train,\n",
        "                     batch_size = batch_size, subset ='validation')\n",
        "        #Entrenamos el modelo\n",
        "        historial=model.fit_generator(generator_train,\n",
        "                            steps_per_epoch = len(x_train)*0.9/32,\n",
        "                            epochs = epocas, \n",
        "                            validation_data = generator_valid, \n",
        "                            validation_steps = len(x_train)*0.1/32,\n",
        "                            callbacks=cb_list)\n",
        "        #Mostramos información del early stopping y el entrenamiento\n",
        "        mejor_epoc = len(historial.epoch) - 15\n",
        "        print(\"\\nNo se ha mejorado en las últimas 15 épocas.\")\n",
        "        print(\"Mejores pesos obtenidos en la época\", mejor_epoc)\n",
        "    #Devolvemos el modelos y el test para calcular el accuracy\n",
        "    #Devolvemos también el historial de entrenamiento\n",
        "    return model, x_test, y_test, historial\n",
        "\n",
        "#########################################################################\n",
        "################ PREDICCIÓN SOBRE EL CONJUNTO DE TEST ###################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\"Muestra el accuracy y loss del modelo\"\"\"\n",
        "def prediccionTest(model, x_test, y_test):\n",
        "    #Evaluamos el modelo\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print(\"BaseNet evaluación\")\n",
        "    #Mostramos la accuracy\n",
        "    print('Test accuracy:', score[1])\n",
        "    #Mostramos la pérdida\n",
        "    print('Test loss:', score[0])\n",
        "\n",
        "#########################################################################\n",
        "########################## MEJORA DEL MODELO ############################\n",
        "#########################################################################\n",
        "\n",
        "\"\"\"Versión mejorada de la arquitectura de modelo BaseNet pedida en la guía\n",
        "de prácticas.\"\"\"\n",
        "def basenetMejorado():\n",
        "    #Modelo Sequential inicial\n",
        "    model = Sequential()\n",
        "    #Hacer una convolución de tamaño 5 es lo mismo que hacer dos de 3 así\n",
        "    #que sustituimos por dos de 2 obteniendo así más profundidad\n",
        "    model.add(Conv2D(32,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3),\n",
        "                     input_shape = INPUT_SHAPE))\n",
        "    #batchNormalization después de cada convolución totalmente conectada\n",
        "    #(menos la última) para mantener la media más o menos a 1 y varianza 0\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32,\n",
        "                     kernel_size = (3, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64,\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    #Añado una convolución más para aumentar la profundidad con padding para\n",
        "    #no cambiar la dimensión de salida\n",
        "    model.add(Conv2D(128,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(50))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(25,activation = 'softmax'))\n",
        "    return model\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "########## Funciones para las ejecuciones de los apartados ##############\n",
        "#########################################################################\n",
        "\n",
        "\"\"\"Ejecución del apartado 1. Instanciamos el modelo y lo entrenamos.\n",
        "Después mostramos su evolución.\"\"\"\n",
        "def apartado1():\n",
        "    print(\"Apartado 1\")\n",
        "    #Instanciamos el modelo báscico de BaseNet\n",
        "    modelo=basenetBasico()\n",
        "    #Compilamos el modelo antes de entrenarlo\n",
        "    modelo=compilar(modelo)\n",
        "    #Entrenamiento del modelo\n",
        "    modelo, xt, yt, historial=entrenarModelo(modelo, epocas=30)\n",
        "    #Función para mostrar la accuracy\n",
        "    prediccionTest(modelo,xt, yt)\n",
        "    #Mostramos la evolución del modelo en el entrenamiento\n",
        "    mostrarEvolucion(historial)\n",
        "\n",
        "\"\"\"Ejecución del apartado 2. Instanciamos el modelo y el datagen y lo entrenamos.\n",
        "Después mostramos su evolución.\"\"\"\n",
        "def apartado2():\n",
        "    input(\"\\n--- Pulsar tecla para continuar. Ejecucion apartado 2 ---\\n\")\n",
        "    #Instanciamos el modelo báscico de BaseNet\n",
        "    modelo=basenetMejorado()\n",
        "    #Compilamos el modelo antes de entrenarlo\n",
        "    modelo=compilar(modelo)\n",
        "    datagen=ImageDataGenerator(featurewise_center = True,\n",
        "                              featurewise_std_normalization = True,\n",
        "                              width_shift_range = 0.1,\n",
        "                              height_shift_range = 0.1,\n",
        "                              zoom_range = 0.2,\n",
        "                              horizontal_flip = True,\n",
        "                              validation_split = 0.1)\n",
        "    #Entrenamiento del modelo\n",
        "    modelo, xt, yt, historial=entrenarModelo(modelo,datagen=datagen,epocas=100)\n",
        "    #Función para mostrar la accuracy\n",
        "    prediccionTest(modelo,xt, yt)\n",
        "    #Mostramos la evolución del modelo en el entrenamiento\n",
        "    mostrarEvolucion(historial)\n",
        "###################\n",
        "\"\"\"Bonus\"\"\"\n",
        "###################\n",
        "\"\"\"Versión mejorada de la arquitectura de modelo BaseNet del apartado 2.\"\"\"\n",
        "def basenetBonus():\n",
        "    #Modelo Sequential inicial\n",
        "    model = Sequential()\n",
        "    #Hacer una convolución de tamaño 5 es lo mismo que hacer dos de 3 así\n",
        "    #que sustituimos por dos de 2 obteniendo así más profundidad\n",
        "    model.add(Conv2D(32,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3),\n",
        "                     input_shape = INPUT_SHAPE))\n",
        "    #batchNormalization después de cada convolución totalmente conectada\n",
        "    #(menos la última) para mantener la media más o menos a 1 y varianza 0\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(32,\n",
        "                     kernel_size = (3, 3)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(64,\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    #Añado una convolución más para aumentar la profundidad con padding para\n",
        "    #no cambiar la dimensión de salida\n",
        "    model.add(Conv2D(128,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(128,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Conv2D(256,\n",
        "                     padding = 'same',\n",
        "                     kernel_size = (3, 3)))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size = (2, 2)))\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(GlobalAveragePooling2D())\n",
        "    model.add(Dense(200))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(50))\n",
        "    #batchNormalization\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(25,activation = 'softmax'))\n",
        "    return model\n",
        "\n",
        "\"\"\"Ejecución del bonus. Instanciamos el modelo y el datagen y lo entrenamos.\n",
        "Después mostramos su evolución. Intentamos mejorar el modelo obtenido en el \n",
        "apartado 2. Para ello añadimos cizallamiento en el datagen y la posibilidad\n",
        "de rotar imágenes.\"\"\"\n",
        "def bonus():\n",
        "    input(\"\\n--- Pulsar tecla para continuar. Ejecucion bonus ---\\n\")\n",
        "    #Instanciamos el modelo báscico de BaseNet\n",
        "    modelo=basenetBonus()\n",
        "    #Compilamos el modelo antes de entrenarlo\n",
        "    modelo=compilar(modelo)\n",
        "    datagen=ImageDataGenerator(featurewise_center = True,\n",
        "                              featurewise_std_normalization = True,\n",
        "                              rotation_range=40,\n",
        "                              width_shift_range = 0.1,\n",
        "                              height_shift_range = 0.1,\n",
        "                              zoom_range = 0.2,\n",
        "                              horizontal_flip = True,\n",
        "                              shear_range=0.2,\n",
        "                              validation_split = 0.1)\n",
        "    #Entrenamiento del modelo\n",
        "    modelo, xt, yt, historial=entrenarModelo(modelo,datagen=datagen,epocas=150)\n",
        "    #Función para mostrar la accuracy\n",
        "    prediccionTest(modelo,xt, yt)\n",
        "    #Mostramos la evolución del modelo en el entrenamiento\n",
        "    mostrarEvolucion(historial)\n",
        "#########################################################################\n",
        "####################### Ejecuciones apartados ###########################\n",
        "#########################################################################\n",
        "\n",
        "\n",
        "apartado1()\n",
        "apartado2()\n",
        "bonus()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}